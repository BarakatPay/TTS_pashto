{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c3207b-c05c-4588-810c-0615253e2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46157d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIX PIL/matplotlib DLL issues on Windows\n",
    "!pip uninstall -y pillow matplotlib\n",
    "!pip install pillow==9.5.0 matplotlib==3.7.2\n",
    "\n",
    "print(\"✅ PIL/matplotlib compatibility fixed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c9609-20c6-448a-93cf-51654ca031cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting soundfile\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting speechbrain\n",
      "  Using cached speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from datasets) (2.0.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp39-cp39-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.0-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.12-cp39-cp39-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Collecting hyperpyyaml (from speechbrain)\n",
      "  Using cached HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting joblib (from speechbrain)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting scipy (from speechbrain)\n",
      "  Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
      "Collecting sentencepiece (from speechbrain)\n",
      "  Using cached sentencepiece-0.2.0-cp39-cp39-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: torch>=1.9 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from speechbrain) (2.5.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from speechbrain) (2.5.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.4.4-cp39-cp39-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp39-cp39-win_amd64.whl.metadata (76 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: pycparser in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: networkx in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from torch>=1.9->speechbrain) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from torch>=1.9->speechbrain) (3.1.6)\n",
      "Collecting sympy==1.13.1 (from torch>=1.9->speechbrain)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from sympy==1.13.1->torch>=1.9->speechbrain) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n",
      "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n",
      "  Downloading ruamel.yaml.clib-0.2.12-cp39-cp39-win_amd64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from jinja2->torch>=1.9->speechbrain) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Using cached speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
      "Downloading aiohttp-3.12.12-cp39-cp39-win_amd64.whl (451 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached multidict-6.4.4-cp39-cp39-win_amd64.whl (38 kB)\n",
      "Downloading yarl-1.20.1-cp39-cp39-win_amd64.whl (87 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.7.0-cp39-cp39-win_amd64.whl (44 kB)\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Downloading propcache-0.3.2-cp39-cp39-win_amd64.whl (42 kB)\n",
      "Downloading pyarrow-20.0.0-cp39-cp39-win_amd64.whl (25.8 MB)\n",
      "   ---------------------------------------- 0.0/25.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.4/25.8 MB 12.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 4.7/25.8 MB 11.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 7.1/25.8 MB 12.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.7/25.8 MB 12.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 12.1/25.8 MB 12.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.4/25.8 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.8/25.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.4/25.8 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.8/25.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.4/25.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.8/25.8 MB 11.8 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
      "Downloading ruamel.yaml.clib-0.2.12-cp39-cp39-win_amd64.whl (118 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading pandas-2.3.0-cp39-cp39-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.4/11.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.7/11.1 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.1/11.1 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.7/11.1 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 11.6 MB/s eta 0:00:00\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.4/46.2 MB 12.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 4.7/46.2 MB 11.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 7.3/46.2 MB 11.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 9.7/46.2 MB 11.8 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 12.1/46.2 MB 11.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 14.7/46.2 MB 11.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 17.0/46.2 MB 11.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 19.4/46.2 MB 11.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 21.8/46.2 MB 11.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 24.4/46.2 MB 11.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 26.5/46.2 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 28.8/46.2 MB 11.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 31.2/46.2 MB 11.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 33.6/46.2 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 35.9/46.2 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 38.5/46.2 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 40.9/46.2 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.3/46.2 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.6/46.2 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 11.6 MB/s eta 0:00:00\n",
      "Using cached sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Installing collected packages: sentencepiece, pytz, xxhash, tzdata, tqdm, sympy, scipy, ruamel.yaml.clib, pyarrow, propcache, multidict, joblib, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, soundfile, ruamel.yaml, pandas, multiprocess, huggingface-hub, aiosignal, hyperpyyaml, aiohttp, speechbrain, datasets\n",
      "\n",
      "   - --------------------------------------  1/28 [pytz]\n",
      "   ---- -----------------------------------  3/28 [tzdata]\n",
      "   ----- ----------------------------------  4/28 [tqdm]\n",
      "  Attempting uninstall: sympy\n",
      "   ----- ----------------------------------  4/28 [tqdm]\n",
      "    Found existing installation: sympy 1.13.3\n",
      "   ----- ----------------------------------  4/28 [tqdm]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "    Uninstalling sympy-1.13.3:\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   ------- --------------------------------  5/28 [sympy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   -------- -------------------------------  6/28 [scipy]\n",
      "   ----------- ----------------------------  8/28 [pyarrow]\n",
      "   ----------- ----------------------------  8/28 [pyarrow]\n",
      "   ----------- ----------------------------  8/28 [pyarrow]\n",
      "   ----------- ----------------------------  8/28 [pyarrow]\n",
      "   ----------- ----------------------------  8/28 [pyarrow]\n",
      "   --------------- ------------------------ 11/28 [joblib]\n",
      "  Attempting uninstall: fsspec\n",
      "   --------------- ------------------------ 11/28 [joblib]\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "   --------------- ------------------------ 11/28 [joblib]\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "   --------------- ------------------------ 11/28 [joblib]\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "   --------------- ------------------------ 11/28 [joblib]\n",
      "   -------------------- ------------------- 14/28 [dill]\n",
      "   --------------------------- ------------ 19/28 [ruamel.yaml]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ---------------------------- ----------- 20/28 [pandas]\n",
      "   ------------------------------ --------- 21/28 [multiprocess]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ----------------------------------- ---- 25/28 [aiohttp]\n",
      "   ------------------------------------- -- 26/28 [speechbrain]\n",
      "   ------------------------------------- -- 26/28 [speechbrain]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   ---------------------------------------- 28/28 [datasets]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.12 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 huggingface-hub-0.33.0 hyperpyyaml-1.2.2 joblib-1.5.1 multidict-6.4.4 multiprocess-0.70.16 pandas-2.3.0 propcache-0.3.2 pyarrow-20.0.0 pytz-2025.2 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 scipy-1.13.1 sentencepiece-0.2.0 soundfile-0.13.1 speechbrain-1.0.3 sympy-1.13.1 tqdm-4.67.1 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to c:\\users\\pc\\appdata\\local\\temp\\pip-req-build-ljh70rui\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 7f00b325f8140c4964e3e81e6af0e53f5b9a2592\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from transformers==4.53.0.dev0) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from transformers==4.53.0.dev0) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from transformers==4.53.0.dev0) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from transformers==4.53.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from transformers==4.53.0.dev0) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.53.0.dev0)\n",
      "  Using cached regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from transformers==4.53.0.dev0) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.53.0.dev0)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.53.0.dev0)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from transformers==4.53.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from tqdm>=4.27->transformers==4.53.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests->transformers==4.53.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests->transformers==4.53.0.dev0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests->transformers==4.53.0.dev0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests->transformers==4.53.0.dev0) (2025.4.26)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml): started\n",
      "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.53.0.dev0-py3-none-any.whl size=11422679 sha256=c5557e8a0711f149e6c2278906da05f81b672a5453940e1d84fac801c8c78d6e\n",
      "  Stored in directory: C:\\Users\\PC\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-s_t829hs\\wheels\\f7\\92\\8c\\752ff3bfcd3439805d8bbf641614da38ef3226e127ebea86ee\n",
      "Successfully built transformers\n",
      "Installing collected packages: safetensors, regex, tokenizers, transformers\n",
      "\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ---------------------------------------- 4/4 [transformers]\n",
      "\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.53.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git 'C:\\Users\\PC\\AppData\\Local\\Temp\\pip-req-build-ljh70rui'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Using cached accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from accelerate) (0.33.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
      "Using cached accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.7.0\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Collecting noisereduce\n",
      "  Using cached noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting librosa\n",
      "  Using cached librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from noisereduce) (1.13.1)\n",
      "Collecting matplotlib (from noisereduce)\n",
      "  Using cached matplotlib-3.9.4-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from noisereduce) (2.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from noisereduce) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from noisereduce) (1.5.1)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Using cached numba-0.60.0-cp39-cp39-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting scikit-learn>=1.1.0 (from librosa)\n",
      "  Using cached scikit_learn-1.6.1-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Using cached soxr-0.5.0.post1-cp39-cp39-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.1-cp39-cp39-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Using cached llvmlite-0.43.0-cp39-cp39-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->noisereduce)\n",
      "  Using cached contourpy-1.3.0-cp39-cp39-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->noisereduce)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->noisereduce)\n",
      "  Downloading fonttools-4.58.2-cp39-cp39-win_amd64.whl.metadata (108 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->noisereduce)\n",
      "  Using cached kiwisolver-1.4.7-cp39-cp39-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from matplotlib->noisereduce) (9.4.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->noisereduce)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib->noisereduce)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->noisereduce) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\anaconda3\\envs\\tts_env\\lib\\site-packages (from tqdm->noisereduce) (0.4.6)\n",
      "Using cached noisereduce-3.0.3-py3-none-any.whl (22 kB)\n",
      "Using cached librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.1-cp39-cp39-win_amd64.whl (71 kB)\n",
      "Using cached numba-0.60.0-cp39-cp39-win_amd64.whl (2.7 MB)\n",
      "Using cached llvmlite-0.43.0-cp39-cp39-win_amd64.whl (28.1 MB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "Using cached soxr-0.5.0.post1-cp39-cp39-win_amd64.whl (167 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading matplotlib-3.9.4-cp39-cp39-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.1/7.8 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.7/7.8 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.1/7.8 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 11.8 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl (211 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.2-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 11.4 MB/s eta 0:00:00\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached kiwisolver-1.4.7-cp39-cp39-win_amd64.whl (55 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: threadpoolctl, soxr, pyparsing, msgpack, llvmlite, lazy_loader, kiwisolver, importlib-resources, fonttools, cycler, contourpy, audioread, scikit-learn, pooch, numba, matplotlib, noisereduce, librosa\n",
      "\n",
      "   -------- -------------------------------  4/18 [llvmlite]\n",
      "   -------- -------------------------------  4/18 [llvmlite]\n",
      "   ----------------- ----------------------  8/18 [fonttools]\n",
      "   ----------------- ----------------------  8/18 [fonttools]\n",
      "   ----------------- ----------------------  8/18 [fonttools]\n",
      "   ----------------- ----------------------  8/18 [fonttools]\n",
      "   ------------------------ --------------- 11/18 [audioread]\n",
      "   -------------------------- ------------- 12/18 [scikit-learn]\n",
      "   -------------------------- ------------- 12/18 [scikit-learn]\n",
      "   -------------------------- ------------- 12/18 [scikit-learn]\n",
      "   -------------------------- ------------- 12/18 [scikit-learn]\n",
      "   -------------------------- ------------- 12/18 [scikit-learn]\n",
      "   -------------------------- ------------- 12/18 [scikit-learn]\n",
      "   -------------------------- ------------- 12/18 [scikit-learn]\n",
      "   -------------------------- ------------- 12/18 [scikit-learn]\n",
      "   -------------------------- ------------- 12/18 [scikit-learn]\n",
      "   ------------------------------- -------- 14/18 [numba]\n",
      "   ------------------------------- -------- 14/18 [numba]\n",
      "   ------------------------------- -------- 14/18 [numba]\n",
      "   ------------------------------- -------- 14/18 [numba]\n",
      "   ------------------------------- -------- 14/18 [numba]\n",
      "   ------------------------------- -------- 14/18 [numba]\n",
      "   ------------------------------- -------- 14/18 [numba]\n",
      "   ------------------------------- -------- 14/18 [numba]\n",
      "   ------------------------------- -------- 14/18 [numba]\n",
      "   --------------------------------- ------ 15/18 [matplotlib]\n",
      "   --------------------------------- ------ 15/18 [matplotlib]\n",
      "   --------------------------------- ------ 15/18 [matplotlib]\n",
      "   --------------------------------- ------ 15/18 [matplotlib]\n",
      "   --------------------------------- ------ 15/18 [matplotlib]\n",
      "   --------------------------------- ------ 15/18 [matplotlib]\n",
      "   ------------------------------------- -- 17/18 [librosa]\n",
      "   ---------------------------------------- 18/18 [librosa]\n",
      "\n",
      "Successfully installed audioread-3.0.1 contourpy-1.3.0 cycler-0.12.1 fonttools-4.58.2 importlib-resources-6.5.2 kiwisolver-1.4.7 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.43.0 matplotlib-3.9.4 msgpack-1.1.1 noisereduce-3.0.3 numba-0.60.0 pooch-1.8.2 pyparsing-3.2.3 scikit-learn-1.6.1 soxr-0.5.0.post1 threadpoolctl-3.6.0\n",
      "Fri Jun 13 15:04:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.52                 Driver Version: 576.52         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5060 Ti   WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   34C    P8              8W /  180W |     581MiB /  16311MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A             436    C+G   ...yb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A             744    C+G   ....0.3296.68\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A            2508    C+G   ...8wekyb3d8bbwe\\WebViewHost.exe      N/A      |\n",
      "|    0   N/A  N/A            5940    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A            6644    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A            7660    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           10380    C+G   ...ntrolPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A           10588    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A           11400    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A           12632    C+G   ....0.3296.68\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           12740    C+G   ...t\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A           16548    C+G   ...al\\Programs\\cursor\\Cursor.exe      N/A      |\n",
      "|    0   N/A  N/A           18828    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           20352    C+G   ...App_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A           20444    C+G   ...crosoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           21288    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
      "|    0   N/A  N/A           21408    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Install required packages with fixed Pillow version\n",
    "!pip install datasets soundfile speechbrain\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install --upgrade accelerate\n",
    "!pip install requests\n",
    "\n",
    "# Fix PIL/matplotlib DLL issues on Windows\n",
    "!pip uninstall -y pillow matplotlib\n",
    "!pip install pillow==9.5.0 \n",
    "!pip install matplotlib==3.7.2\n",
    "\n",
    "!pip install noisereduce librosa\n",
    "\n",
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78876d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42bb93cb-7be6-40cc-990b-9c1b34b4f29a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _imaging: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtempfile\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\tts_env\\lib\\site-packages\\matplotlib\\__init__.py:159\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\tts_env\\lib\\site-packages\\matplotlib\\rcsetup.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\tts_env\\lib\\site-packages\\matplotlib\\colors.py:52\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Real\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPngImagePlugin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PngInfo\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmpl\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\tts_env\\lib\\site-packages\\PIL\\Image.py:103\u001b[0m\n\u001b[0;32m     94\u001b[0m MAX_IMAGE_PIXELS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _imaging \u001b[38;5;28;01mas\u001b[39;00m core\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __version__ \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(core, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    106\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCore version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(core,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPillow version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _imaging: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "# import matplotlib.pyplot as plt  # Removed due to PIL DLL issues\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from IPython.display import Audio\n",
    "import soundfile as sf\n",
    "\n",
    "from transformers import (\n",
    "    SpeechT5Processor,\n",
    "    SpeechT5ForTextToSpeech,\n",
    "    SpeechT5HifiGan,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from datasets import Dataset, Audio as AudioFeature\n",
    "\n",
    "# For denoising and trimming\n",
    "import noisereduce as nr\n",
    "import librosa\n",
    "\n",
    "# Import LocalStrategy to avoid symlink errors on Windows\n",
    "from speechbrain.utils.fetching import LocalStrategy\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "# Load SpeechT5 model + processor\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "\n",
    "# Load Pashto dataset\n",
    "def load_pashto_dataset(json_file_path: str) -> Dataset:\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    dataset_dict = {\n",
    "        'id': [],\n",
    "        'audio_url': [],\n",
    "        'text': [],\n",
    "        'gender': [],\n",
    "        'accent': [],\n",
    "        'speaker_id': []\n",
    "    }\n",
    "    \n",
    "    for item in data:\n",
    "        dataset_dict['id'].append(item['id'])\n",
    "        dataset_dict['audio_url'].append(item['file_url'])\n",
    "        dataset_dict['text'].append(item['sentence'])\n",
    "        dataset_dict['gender'].append(item['gender'])\n",
    "        dataset_dict['accent'].append(item['accent'])\n",
    "        dataset_dict['speaker_id'].append(f\"{item['gender']}_{item['accent'].replace(' ', '_')}\")\n",
    "    \n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "dataset_path = r\"C:\\Users\\PC\\Music\\jj\\new3.json\"  # Replace with your actual path\n",
    "dataset = load_pashto_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c77c57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Original dataset size: 10000 samples\n",
      "🎯 Dataset LIMITED to 1000 samples for faster training and testing\n",
      "✅ This ensures we only process 1000 audio files maximum\n"
     ]
    }
   ],
   "source": [
    "# 🎯 LIMIT DATASET TO 1000 SAMPLES FOR FASTER TRAINING\n",
    "print(f\"📋 Original dataset size: {len(dataset)} samples\")\n",
    "dataset = dataset.select(range(min(1000, len(dataset))))\n",
    "print(f\"🎯 Dataset LIMITED to {len(dataset)} samples for faster training and testing\")\n",
    "print(f\"✅ This ensures we only process 1000 audio files maximum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c89fdfaa-1fdc-4e58-a314-9f66da50badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Original dataset size: 1000 samples\n",
      "🎯 Dataset LIMITED to 1000 samples for faster training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf6b24fb3224723be11b21f5d9701c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee8fd48832449398998194c3fdc5cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Final dataset size after filtering: 1000 audio files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efcd58c5b984d558c8ade8ee3fd1536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b857c93f8744342a4b3ba72e62d58ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added 63 new tokens including prosodic markers\n",
      "📝 Prosodic tokens added: ['<MAJOR_BREAK>', '<PHRASE_BREAK>', '<MINOR_BREAK>', '<STRESS>', '<UNSTRESS>']\n",
      "🔤 Updated vocabulary size: 144\n",
      "🎵 This phrase-level approach should reduce choppiness in Pashto TTS!\n"
     ]
    }
   ],
   "source": [
    "# 🎯 FIRST: Limit dataset to 1000 samples BEFORE expensive audio processing\n",
    "print(f\"📋 Original dataset size: {len(dataset)} samples\")\n",
    "dataset = dataset.select(range(min(1000, len(dataset))))\n",
    "print(f\"🎯 Dataset LIMITED to {len(dataset)} samples for faster training\")\n",
    "\n",
    "# Load audio files from local folder\n",
    "LOCAL_AUDIO_DIR = r\"C:\\Users\\PC\\Downloads\\AudioFiles\"\n",
    "\n",
    "def load_local_audio(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    filename = os.path.basename(example['audio_url'])\n",
    "    local_path = os.path.join(LOCAL_AUDIO_DIR, filename)\n",
    "    \n",
    "    if os.path.isfile(local_path):\n",
    "        try:\n",
    "            audio_array, sample_rate = sf.read(local_path)\n",
    "            example['audio'] = {'array': audio_array, 'sampling_rate': sample_rate}\n",
    "        except Exception as e:\n",
    "            example['audio'] = None\n",
    "    else:\n",
    "        example['audio'] = None\n",
    "    \n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(load_local_audio)\n",
    "dataset = dataset.filter(lambda x: x['audio'] is not None)\n",
    "print(f\"📊 Final dataset size after filtering: {len(dataset)} audio files\")\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", AudioFeature(sampling_rate=16000))\n",
    "\n",
    "# Enhanced text processing with prosodic-aware tokenization for Pashto TTS\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Add Pashto-specific replacements with prosodic-aware processing\n",
    "def cleanup_text(inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    text = inputs[\"text\"]\n",
    "    \n",
    "    # Pashto character replacements\n",
    "    replacements = [\n",
    "        ('ښ', 'ښ'),  # Ensure consistent shin\n",
    "        ('ګ', 'ګ'),  # Ensure consistent gaf\n",
    "        ('ځ', 'ځ'),  # Ensure consistent dzal\n",
    "        ('څ', 'څ'),  # Ensure consistent tse\n",
    "        ('ډ', 'ډ'),  # Ensure consistent dal\n",
    "        ('ړ', 'ړ'),  # Ensure consistent re\n",
    "        ('ټ', 'ټ'),  # Ensure consistent te\n",
    "        ('ږ', 'ږ'),  # Ensure consistent zhge\n",
    "        ('ڼ', 'ڼ'),  # Ensure consistent noon\n",
    "        ('ې', 'ې'),  # Ensure consistent ye\n",
    "        ('ۍ', 'ۍ'),  # Ensure consistent ye\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        (r'\\s+', ' '),\n",
    "        \n",
    "        # Normalize punctuation\n",
    "        ('،', '،'),\n",
    "        ('؟', '؟'),\n",
    "        ('؛', '؛'),\n",
    "        \n",
    "        # Remove problematic characters\n",
    "        ('\\u200c', ''),  # Zero-width non-joiner\n",
    "        ('\\u200d', ''),  # Zero-width joiner\n",
    "        ('\\u200e', ''),  # Left-to-right mark\n",
    "        ('\\u200f', ''),  # Right-to-left mark\n",
    "    ]\n",
    "    \n",
    "    for src, dst in replacements:\n",
    "        text = text.replace(src, dst)\n",
    "    \n",
    "    # Create phrase-aware text for better prosody\n",
    "    phrase_text = create_phrase_aware_text(text)\n",
    "    \n",
    "    inputs[\"normalized_text\"] = text\n",
    "    inputs[\"phrase_text\"] = phrase_text\n",
    "    return inputs\n",
    "\n",
    "def create_phrase_aware_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Create phrase-aware text that maintains prosodic boundaries for Pashto TTS.\n",
    "    This addresses the choppy audio issue by preserving natural speech rhythm.\n",
    "    \"\"\"\n",
    "    # Pashto prosodic boundary markers\n",
    "    # Major boundaries (sentence level)\n",
    "    text = re.sub(r'[؟!۔]', ' <MAJOR_BREAK> ', text)\n",
    "    \n",
    "    # Medium boundaries (phrase level) \n",
    "    text = re.sub(r'[،؛]', ' <PHRASE_BREAK> ', text)\n",
    "    \n",
    "    # Minor boundaries (word group level) - every 2-3 words for Pashto rhythm\n",
    "    words = text.split()\n",
    "    phrase_words = []\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        phrase_words.append(word)\n",
    "        # Add minor breaks every 2-3 words to maintain natural Pashto rhythm\n",
    "        if (i + 1) % 3 == 0 and i < len(words) - 1:\n",
    "            if word not in ['<MAJOR_BREAK>', '<PHRASE_BREAK>']:\n",
    "                phrase_words.append('<MINOR_BREAK>')\n",
    "    \n",
    "    phrase_text = ' '.join(phrase_words)\n",
    "    \n",
    "    # Clean up multiple breaks\n",
    "    phrase_text = re.sub(r'(<[^>]+>\\s*){2,}', r'<PHRASE_BREAK> ', phrase_text)\n",
    "    phrase_text = re.sub(r'\\s+', ' ', phrase_text).strip()\n",
    "    \n",
    "    return phrase_text\n",
    "\n",
    "dataset = dataset.map(cleanup_text)\n",
    "\n",
    "# Enhanced tokenizer setup with prosodic tokens\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# Add prosodic boundary tokens for better speech flow\n",
    "prosodic_tokens = ['<MAJOR_BREAK>', '<PHRASE_BREAK>', '<MINOR_BREAK>', '<STRESS>', '<UNSTRESS>']\n",
    "\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"text\"] + batch[\"phrase_text\"])\n",
    "    chars = list(set(all_text))\n",
    "    return {\"vocab\": [chars], \"all_text\": [all_text]}\n",
    "\n",
    "vocabs = dataset.map(\n",
    "    extract_all_chars,\n",
    "    batched=True,\n",
    "    batch_size=-1,\n",
    "    keep_in_memory=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
    "tokenizer_vocab = set(tokenizer.get_vocab().keys())\n",
    "\n",
    "# Add missing characters and prosodic tokens\n",
    "missing_chars = dataset_vocab - tokenizer_vocab\n",
    "all_new_tokens = list(missing_chars) + prosodic_tokens\n",
    "\n",
    "if all_new_tokens:\n",
    "    tokenizer.add_tokens(all_new_tokens)\n",
    "    processor.tokenizer = tokenizer\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "print(f\"✅ Added {len(all_new_tokens)} new tokens including prosodic markers\")\n",
    "print(f\"📝 Prosodic tokens added: {prosodic_tokens}\")\n",
    "print(f\"🔤 Updated vocabulary size: {len(tokenizer)}\")\n",
    "print(\"🎵 This phrase-level approach should reduce choppiness in Pashto TTS!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2fcc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fcaf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ceb16-f688-459d-8a46-80cf3e94161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load speaker model ONCE outside the function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\"audio\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\"phrase_text\", example[\"normalized_text\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\"labels\"] = processed[\"labels\"][0]\n",
    "    example[\"input_ids\"] = processed[\"input_ids\"]\n",
    "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\"input_ids\"], tokenizer)\n",
    "    example[\"prosodic_features\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "def create_prosodic_features(input_ids: torch.Tensor, tokenizer) -> torch.Tensor:\n",
    "    \"\"\"Create prosodic features based on break tokens to guide TTS synthesis\"\"\"\n",
    "    prosodic_features = torch.zeros(len(input_ids))\n",
    "    \n",
    "    # Get token IDs for prosodic markers\n",
    "    break_token_ids = {\n",
    "        tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0] if '<MAJOR_BREAK>' in tokenizer.get_vocab() else -1,\n",
    "        tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0] if '<PHRASE_BREAK>' in tokenizer.get_vocab() else -1,\n",
    "        tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0] if '<MINOR_BREAK>' in tokenizer.get_vocab() else -1,\n",
    "    }\n",
    "    break_token_ids.discard(-1)  # Remove invalid tokens\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids):\n",
    "        if token_id.item() in break_token_ids:\n",
    "            # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "            if '<MAJOR_BREAK>' in tokenizer.get_vocab() and token_id == tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]:\n",
    "                prosodic_features[i] = 1.0\n",
    "            elif '<PHRASE_BREAK>' in tokenizer.get_vocab() and token_id == tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]:\n",
    "                prosodic_features[i] = 0.7\n",
    "            elif '<MINOR_BREAK>' in tokenizer.get_vocab() and token_id == tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]:\n",
    "                prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "# Now run dataset mapping - will be ~100x faster\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=[\"audio\", \"audio_url\", \"text\", \"gender\", \"accent\", \"id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09f867ce-55ab-4c22-825e-953f016dadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Data Collator with Prosodic Features\n",
    "@dataclass\n",
    "class TTSDataCollatorEnhanced:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract input components\n",
    "        input_ids = [{\"input_ids\": f[\"input_ids\"]} for f in features]\n",
    "        label_features = [{\"input_values\": f[\"labels\"]} for f in features]\n",
    "        speaker_features = [f[\"speaker_embeddings\"] for f in features]\n",
    "        \n",
    "        # Handle prosodic features\n",
    "        prosodic_features = [f.get(\"prosodic_features\", torch.zeros(len(f[\"input_ids\"]))) for f in features]\n",
    "        \n",
    "        # Pad sequences\n",
    "        batch = self.processor.pad(\n",
    "            input_ids=input_ids,\n",
    "            labels=label_features,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Mask labels for padding\n",
    "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
    "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
    "        )\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "\n",
    "        # Handle reduction factor\n",
    "        if model.config.reduction_factor > 1:\n",
    "            target_lengths = torch.tensor([len(f[\"input_values\"]) for f in label_features])\n",
    "            target_lengths = target_lengths.new([\n",
    "                length - length % model.config.reduction_factor\n",
    "                for length in target_lengths\n",
    "            ])\n",
    "            max_length = max(target_lengths).item()\n",
    "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
    "\n",
    "        # Add speaker embeddings\n",
    "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
    "        \n",
    "        # Pad prosodic features to match input sequence length\n",
    "        max_seq_len = batch[\"input_ids\"].size(1)\n",
    "        padded_prosodic = []\n",
    "        \n",
    "        for prosodic in prosodic_features:\n",
    "            if len(prosodic) < max_seq_len:\n",
    "                # Pad with zeros\n",
    "                padded = torch.cat([prosodic, torch.zeros(max_seq_len - len(prosodic))])\n",
    "            else:\n",
    "                # Truncate if too long\n",
    "                padded = prosodic[:max_seq_len]\n",
    "            padded_prosodic.append(padded)\n",
    "        \n",
    "        batch[\"prosodic_features\"] = torch.stack(padded_prosodic)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# Initialize enhanced data collator\n",
    "data_collator = TTSDataCollatorEnhanced(processor=processor)\n",
    "print(\"Enhanced data collator initialized with prosodic feature support!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c37d689b-f1e6-4eea-85b8-202b58b30300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())  # Should print True if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "044c0e40-c0e5-4733-a788-06d82e12ffd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 9000\n",
      "Validation dataset size: 1000\n",
      "Expected steps per epoch: 2250\n",
      "Loading model weights from: C:\\Users\\PC\\speecht5_tts_pashto\\checkpoint-11250\n",
      "Model weights loaded with vocab size: 170\n",
      "Starting fresh training with loaded model weights...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11250' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11250/11250 1:10:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.551322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.470300</td>\n",
       "      <td>0.552072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.464300</td>\n",
       "      <td>0.550848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.471000</td>\n",
       "      <td>0.548769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.469500</td>\n",
       "      <td>0.547081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.469900</td>\n",
       "      <td>0.553332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.466900</td>\n",
       "      <td>0.547237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.464100</td>\n",
       "      <td>0.543017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.546820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.467500</td>\n",
       "      <td>0.545039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.542023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.543866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.462900</td>\n",
       "      <td>0.543403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.458700</td>\n",
       "      <td>0.543348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.458300</td>\n",
       "      <td>0.543178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.460400</td>\n",
       "      <td>0.541352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.458600</td>\n",
       "      <td>0.541598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.460400</td>\n",
       "      <td>0.541100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.540029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.542773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.455300</td>\n",
       "      <td>0.540088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.455400</td>\n",
       "      <td>0.537416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.457600</td>\n",
       "      <td>0.539209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.454100</td>\n",
       "      <td>0.535999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.456300</td>\n",
       "      <td>0.540024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.459200</td>\n",
       "      <td>0.537488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.455200</td>\n",
       "      <td>0.538556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.458400</td>\n",
       "      <td>0.539568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.537480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.453700</td>\n",
       "      <td>0.535508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.458500</td>\n",
       "      <td>0.536035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.452200</td>\n",
       "      <td>0.537964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.453900</td>\n",
       "      <td>0.539742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.455700</td>\n",
       "      <td>0.535297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.455800</td>\n",
       "      <td>0.536978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.535240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.456800</td>\n",
       "      <td>0.537067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.450800</td>\n",
       "      <td>0.533898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.535032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.455700</td>\n",
       "      <td>0.536249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.535355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.450800</td>\n",
       "      <td>0.535931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.454100</td>\n",
       "      <td>0.535137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.452100</td>\n",
       "      <td>0.534564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>0.533625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.536099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.450100</td>\n",
       "      <td>0.536272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.448500</td>\n",
       "      <td>0.535534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.536621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.451200</td>\n",
       "      <td>0.536063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.535922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.449800</td>\n",
       "      <td>0.532923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.453600</td>\n",
       "      <td>0.535511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.533798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.535098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "import os\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, SpeechT5ForTextToSpeech\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# ADD THIS LINE - Split the dataset first\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"Training dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "# Calculate expected steps\n",
    "samples_per_step = 4  # batch_size * gradient_accumulation_steps  \n",
    "steps_per_epoch = len(dataset['train']) // samples_per_step\n",
    "print(f\"Expected steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "# Load model weights from checkpoint but start training from scratch\n",
    "checkpoint_base = r\"C:\\Users\\PC\\speecht5_tts_pashto\"\n",
    "latest_checkpoint = None\n",
    "\n",
    "if os.path.isdir(checkpoint_base):\n",
    "    ckpts = [d for d in os.listdir(checkpoint_base)\n",
    "             if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(checkpoint_base, d))]\n",
    "    if ckpts:\n",
    "        latest = max(ckpts, key=lambda x: int(x.split(\"-\", 1)[1]))\n",
    "        latest_checkpoint = os.path.join(checkpoint_base, latest)\n",
    "        print(f\"Loading model weights from: {latest_checkpoint}\")\n",
    "        \n",
    "        # Load only the model weights, not the training state\n",
    "        model = SpeechT5ForTextToSpeech.from_pretrained(latest_checkpoint)\n",
    "        model = model.to('cuda')\n",
    "        print(f\"Model weights loaded with vocab size: {model.config.vocab_size}\")\n",
    "    else:\n",
    "        print(\"No checkpoints found, using current model\")\n",
    "        model = model.to('cuda')\n",
    "else:\n",
    "    print(\"No checkpoint directory found, using current model\")\n",
    "    model = model.to('cuda')\n",
    "\n",
    "# Fresh training arguments - no resume_from_checkpoint\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=checkpoint_base,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-6,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=5,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,\n",
    "    logging_steps=200,\n",
    "    report_to=[],\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"labels\"],\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor,\n",
    ")\n",
    "\n",
    "print(\"Starting fresh training with loaded model weights...\")\n",
    "trainer.train()\n",
    "print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e109547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c1f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e45023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d33725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b17d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e0a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbffced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4dd249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231dcab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36453a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea6e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd8917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIXED SPEAKER EMBEDDING AND DATASET PREPARATION - Resolved .item() error\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_cache = os.path.join(tempfile.gettempdir(), spk_model_name)\n",
    "\n",
    "print(f\"💻 Using device: {device}\")\n",
    "print(\"🔄 Loading speaker model...\")\n",
    "\n",
    "# GLOBAL MODEL - loaded once\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=speaker_cache,\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ")\n",
    "print(\"✅ Speaker model loaded successfully!\")\n",
    "\n",
    "def create_speaker_embedding(waveform: List[float]) -> Any:\n",
    "    # Use the already loaded global model\n",
    "    with torch.no_grad():\n",
    "        embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=2)\n",
    "        return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def create_prosodic_features(input_ids, tokenizer) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"🎵 FIXED: Create prosodic features based on break tokens to guide TTS synthesis\\\"\\\"\\\"\n",
    "    # Convert to tensor if it's a list, otherwise keep as is\n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "    \n",
    "    prosodic_features = torch.zeros(len(input_ids_tensor))\n",
    "    \n",
    "    # Get token IDs for prosodic markers safely\n",
    "    break_token_ids = {}\n",
    "    try:\n",
    "        if '<MAJOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['major'] = tokenizer.encode('<MAJOR_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<PHRASE_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['phrase'] = tokenizer.encode('<PHRASE_BREAK>', add_special_tokens=False)[0]\n",
    "        if '<MINOR_BREAK>' in tokenizer.get_vocab():\n",
    "            break_token_ids['minor'] = tokenizer.encode('<MINOR_BREAK>', add_special_tokens=False)[0]\n",
    "    except (IndexError, KeyError):\n",
    "        # If encoding fails, return zeros (no prosodic features)\n",
    "        return prosodic_features\n",
    "    \n",
    "    for i, token_id in enumerate(input_ids_tensor):\n",
    "        # 🔧 FIXED: Handle both tensor and int token_id properly\n",
    "        token_val = token_id.item() if hasattr(token_id, 'item') else int(token_id)\n",
    "        \n",
    "        # Assign prosodic strength: major=1.0, phrase=0.7, minor=0.3\n",
    "        if 'major' in break_token_ids and token_val == break_token_ids['major']:\n",
    "            prosodic_features[i] = 1.0\n",
    "        elif 'phrase' in break_token_ids and token_val == break_token_ids['phrase']:\n",
    "            prosodic_features[i] = 0.7\n",
    "        elif 'minor' in break_token_ids and token_val == break_token_ids['minor']:\n",
    "            prosodic_features[i] = 0.3\n",
    "    \n",
    "    return prosodic_features\n",
    "\n",
    "def prepare_dataset(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    audio = example[\\\"audio\\\"]\n",
    "    \n",
    "    # Use phrase_text for better prosodic flow instead of normalized_text\n",
    "    text_to_use = example.get(\\\"phrase_text\\\", example[\\\"normalized_text\\\"])\n",
    "    \n",
    "    processed = processor(\n",
    "        text=text_to_use,\n",
    "        audio_target=audio[\\\"array\\\"],\n",
    "        sampling_rate=audio[\\\"sampling_rate\\\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\\\"labels\\\"] = processed[\\\"labels\\\"][0]\n",
    "    example[\\\"input_ids\\\"] = processed[\\\"input_ids\\\"]\n",
    "    example[\\\"speaker_embeddings\\\"] = create_speaker_embedding(audio[\\\"array\\\"])\n",
    "    \n",
    "    # Add prosodic features based on break tokens\n",
    "    prosodic_features = create_prosodic_features(processed[\\\"input_ids\\\"], tokenizer)\n",
    "    example[\\\"prosodic_features\\\"] = prosodic_features\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\\\"🚀 Starting dataset preparation with 1000 samples...\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4da57e9c-7846-47c0-9ea7-8776556ca96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved as 'pashto_output_trimmed.wav'\n"
     ]
    }
   ],
   "source": [
    "# Save model and processor\n",
    "trainer.save_model(\"./speecht5_tts_pashto_final\")\n",
    "processor.save_pretrained(\"./speecht5_tts_pashto_final\")\n",
    "\n",
    "# Inference with the trained model\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"./speecht5_tts_pashto_final\")\n",
    "\n",
    "# Load speaker embeddings (use the same one from training)\n",
    "import datasets\n",
    "embeddings_dataset = datasets.load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "   model = model.to('cuda')\n",
    "   vocoder = vocoder.to('cuda')\n",
    "   speaker_embeddings = speaker_embeddings.to('cuda')\n",
    "\n",
    "test_text = \"صبر، د زغم نښه ده!\"  # \"Hello brother, how are you?\"\n",
    "inputs = processor(text=test_text, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to same device as model\n",
    "if torch.cuda.is_available():\n",
    "   inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "# Generate speech\n",
    "with torch.no_grad():\n",
    "   spectrogram = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings)\n",
    "   speech = vocoder(spectrogram)\n",
    "\n",
    "# Move back to CPU for processing\n",
    "waveform = speech.squeeze().cpu().numpy()\n",
    "sr = 16000\n",
    "\n",
    "# Trim silence\n",
    "trimmed, _ = librosa.effects.trim(waveform)\n",
    "\n",
    "# Playback and save\n",
    "display(Audio(trimmed, rate=sr))\n",
    "sf.write(\"pashto_output_trimmed.wav\", trimmed, samplerate=sr)\n",
    "print(\"Audio saved as 'pashto_output_trimmed.wav'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167308f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎵 COMPARISON: Word-level vs Phrase-level Tokenization for Pashto TTS\n",
    "# This demonstrates why phrase-level tokenization reduces choppiness\n",
    "\n",
    "# Test with the same Pashto sentence using both approaches\n",
    "test_sentence = \"زه ډېر خوشحاله یم چې دلته یم، ښه راغلاست!\"  # \"I am very happy to be here, welcome!\"\n",
    "\n",
    "print(\"🔍 TOKENIZATION COMPARISON FOR PASHTO TTS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Original word-level approach (choppy)\n",
    "word_tokens = processor.tokenizer.tokenize(test_sentence)\n",
    "print(\"❌ Word-level tokens (causes choppiness):\")\n",
    "print(f\"   {word_tokens}\")\n",
    "print(f\"   Length: {len(word_tokens)} tokens\")\n",
    "\n",
    "# Our improved phrase-level approach\n",
    "phrase_sentence = create_phrase_aware_text(test_sentence)\n",
    "phrase_tokens = processor.tokenizer.tokenize(phrase_sentence)\n",
    "print(f\"\\n✅ Phrase-level tokens (better prosody):\")\n",
    "print(f\"   {phrase_tokens}\")\n",
    "print(f\"   Length: {len(phrase_tokens)} tokens\")\n",
    "\n",
    "print(f\"\\n📝 Original: {test_sentence}\")\n",
    "print(f\"📝 With breaks: {phrase_sentence}\")\n",
    "\n",
    "# Generate audio with improved approach\n",
    "inputs = processor(text=phrase_sentence, return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    spectrogram = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings)\n",
    "    speech = vocoder(spectrogram)\n",
    "\n",
    "# Process and save improved audio\n",
    "waveform = speech.squeeze().cpu().numpy()\n",
    "trimmed, _ = librosa.effects.trim(waveform)\n",
    "\n",
    "# Apply light noise reduction for cleaner output\n",
    "if len(trimmed) > 1000:  # Only if audio is long enough\n",
    "    trimmed = nr.reduce_noise(y=trimmed, sr=16000, prop_decrease=0.8)\n",
    "\n",
    "display(Audio(trimmed, rate=16000))\n",
    "sf.write(\"pashto_improved_prosody.wav\", trimmed, samplerate=16000)\n",
    "\n",
    "print(\"\\n🎉 SOLUTION SUMMARY:\")\n",
    "print(\"✅ Phrase-level tokenization maintains natural Pashto rhythm\")\n",
    "print(\"✅ Prosodic break tokens guide speech synthesis timing\") \n",
    "print(\"✅ Reduces choppy artifacts from word-level boundaries\")\n",
    "print(\"✅ Audio saved as 'pashto_improved_prosody.wav'\")\n",
    "print(\"\\n💡 The key insight: Pashto TTS needs phrase-level context, not just word-level!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tts_env)",
   "language": "python",
   "name": "tts_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
