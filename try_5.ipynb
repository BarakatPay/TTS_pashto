{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocabulary size: 50\n",
      "Validating dataset...\n",
      "Valid samples: 9753 out of 10000\n",
      "Training samples: 8777\n",
      "Validation samples: 976\n",
      "Model parameters: 13,809,617\n",
      "\n",
      "Epoch 1/100\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PashtoTextProcessor:\n",
    "    \"\"\"Text preprocessing for Pashto language\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Pashto alphabet and common characters\n",
    "        self.pashto_chars = [\n",
    "            'ا', 'آ', 'ب', 'پ', 'ت', 'ټ', 'ث', 'ج', 'چ', 'ح', 'خ', 'د', 'ډ', 'ذ', 'ر', 'ړ', 'ز', 'ژ', 'س', 'ش', 'ښ',\n",
    "            'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ک', 'ګ', 'ل', 'م', 'ن', 'ڼ', 'و', 'ه', 'ي', 'ۍ', 'ې',\n",
    "            ' ', '.', '،', '؟', '!', '\\n'\n",
    "        ]\n",
    "        \n",
    "        # Create character mappings\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(self.pashto_chars)}\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(self.pashto_chars)}\n",
    "        self.vocab_size = len(self.pashto_chars)\n",
    "        \n",
    "        # Add special tokens\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.sos_token = '<SOS>'\n",
    "        self.eos_token = '<EOS>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        \n",
    "        special_tokens = [self.pad_token, self.sos_token, self.eos_token, self.unk_token]\n",
    "        for token in special_tokens:\n",
    "            self.char_to_idx[token] = len(self.char_to_idx)\n",
    "            self.idx_to_char[len(self.idx_to_char)] = token\n",
    "        \n",
    "        self.vocab_size = len(self.char_to_idx)\n",
    "        self.pad_idx = self.char_to_idx[self.pad_token]\n",
    "        self.sos_idx = self.char_to_idx[self.sos_token]\n",
    "        self.eos_idx = self.char_to_idx[self.eos_token]\n",
    "        self.unk_idx = self.char_to_idx[self.unk_token]\n",
    "    \n",
    "    def text_to_sequence(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert text to sequence of indices\"\"\"\n",
    "        # Clean text\n",
    "        text = text.strip().replace('\\n', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Convert to indices\n",
    "        sequence = [self.sos_idx]\n",
    "        for char in text:\n",
    "            if char in self.char_to_idx:\n",
    "                sequence.append(self.char_to_idx[char])\n",
    "            else:\n",
    "                sequence.append(self.unk_idx)\n",
    "        sequence.append(self.eos_idx)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def sequence_to_text(self, sequence: List[int]) -> str:\n",
    "        \"\"\"Convert sequence of indices back to text\"\"\"\n",
    "        chars = []\n",
    "        for idx in sequence:\n",
    "            if idx in self.idx_to_char and idx not in [self.pad_idx, self.sos_idx, self.eos_idx]:\n",
    "                chars.append(self.idx_to_char[idx])\n",
    "        return ''.join(chars)\n",
    "\n",
    "class AudioProcessor:\n",
    "    \"\"\"Audio preprocessing for TTS\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=22050, n_mels=80, hop_length=256, win_length=1024):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.n_fft = win_length\n",
    "        \n",
    "        # Mel spectrogram transformer\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=self.n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "            power=2.0\n",
    "        )\n",
    "    \n",
    "    def load_audio(self, audio_path: str) -> torch.Tensor:\n",
    "        \"\"\"Load and preprocess audio file\"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            \n",
    "            # Resample if necessary\n",
    "            if sr != self.sample_rate:\n",
    "                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "                waveform = resampler(waveform)\n",
    "            \n",
    "            # Convert to mono if stereo\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            \n",
    "            return waveform.squeeze(0)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio {audio_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def audio_to_mel(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert waveform to mel spectrogram\"\"\"\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        # Convert to log scale\n",
    "        mel_spec = torch.log(mel_spec + 1e-8)\n",
    "        return mel_spec.squeeze(0).T  # (time, n_mels)\n",
    "    \n",
    "    def normalize_mel(self, mel_spec: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Normalize mel spectrogram\"\"\"\n",
    "        return (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-8)\n",
    "\n",
    "class PashtoTTSDataset(Dataset):\n",
    "    \"\"\"Dataset class for Pashto TTS training\"\"\"\n",
    "    \n",
    "    def __init__(self, json_path: str, audio_base_path: str, text_processor: PashtoTextProcessor, \n",
    "                 audio_processor: AudioProcessor, max_text_len: int = 200):\n",
    "        \n",
    "        self.text_processor = text_processor\n",
    "        self.audio_processor = audio_processor\n",
    "        self.max_text_len = max_text_len\n",
    "        self.audio_base_path = audio_base_path\n",
    "        \n",
    "        # Load data\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        # Filter and validate data\n",
    "        self.valid_samples = []\n",
    "        print(\"Validating dataset...\")\n",
    "        \n",
    "        for item in self.data:\n",
    "            audio_path = os.path.join(audio_base_path, item['file'])\n",
    "            if os.path.exists(audio_path):\n",
    "                # Check if text is not too long\n",
    "                text_seq = self.text_processor.text_to_sequence(item['sentence'])\n",
    "                if len(text_seq) <= max_text_len:\n",
    "                    self.valid_samples.append({\n",
    "                        'id': item['id'],\n",
    "                        'audio_path': audio_path,\n",
    "                        'text': item['sentence'],\n",
    "                        'gender': item.get('gender', 'Unknown'),\n",
    "                        'accent': item.get('accent', 'Unknown')\n",
    "                    })\n",
    "        \n",
    "        print(f\"Valid samples: {len(self.valid_samples)} out of {len(self.data)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.valid_samples[idx]\n",
    "        \n",
    "        # Load and process audio\n",
    "        waveform = self.audio_processor.load_audio(sample['audio_path'])\n",
    "        if waveform is None:\n",
    "            # Return a dummy sample if audio loading fails\n",
    "            mel_spec = torch.zeros(100, self.audio_processor.n_mels)\n",
    "        else:\n",
    "            mel_spec = self.audio_processor.audio_to_mel(waveform)\n",
    "            mel_spec = self.audio_processor.normalize_mel(mel_spec)\n",
    "        \n",
    "        # Process text\n",
    "        text_sequence = self.text_processor.text_to_sequence(sample['text'])\n",
    "        \n",
    "        return {\n",
    "            'id': sample['id'],\n",
    "            'text_sequence': torch.tensor(text_sequence, dtype=torch.long),\n",
    "            'mel_spectrogram': mel_spec,\n",
    "            'text': sample['text'],\n",
    "            'gender': sample['gender'],\n",
    "            'accent': sample['accent']\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for batching\"\"\"\n",
    "    \n",
    "    # Sort by text length (for better training)\n",
    "    batch = sorted(batch, key=lambda x: len(x['text_sequence']), reverse=True)\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_sequences = [item['text_sequence'] for item in batch]\n",
    "    mel_spectrograms = [item['mel_spectrogram'] for item in batch]\n",
    "    \n",
    "    # Pad text sequences\n",
    "    text_lengths = [len(seq) for seq in text_sequences]\n",
    "    padded_texts = pad_sequence(text_sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Pad mel spectrograms\n",
    "    mel_lengths = [mel.shape[0] for mel in mel_spectrograms]\n",
    "    max_mel_len = max(mel_lengths)\n",
    "    \n",
    "    padded_mels = torch.zeros(len(batch), max_mel_len, mel_spectrograms[0].shape[1])\n",
    "    for i, mel in enumerate(mel_spectrograms):\n",
    "        padded_mels[i, :mel.shape[0], :] = mel\n",
    "    \n",
    "    return {\n",
    "        'text_sequences': padded_texts,\n",
    "        'text_lengths': torch.tensor(text_lengths),\n",
    "        'mel_spectrograms': padded_mels,\n",
    "        'mel_lengths': torch.tensor(mel_lengths),\n",
    "        'ids': [item['id'] for item in batch],\n",
    "        'texts': [item['text'] for item in batch]\n",
    "    }\n",
    "\n",
    "class TTSEncoder(nn.Module):\n",
    "    \"\"\"Text encoder for TTS model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.projection = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, text_sequences, text_lengths):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(text_sequences)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(packed)\n",
    "        \n",
    "        # Unpack\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        \n",
    "        # Projection\n",
    "        output = self.projection(unpacked)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TTSDecoder(nn.Module):\n",
    "    \"\"\"Mel spectrogram decoder\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=512, mel_dim=80, attention_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mel_dim = mel_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention_query = nn.Linear(hidden_dim, attention_dim)\n",
    "        self.attention_key = nn.Linear(hidden_dim, attention_dim)\n",
    "        self.attention_value = nn.Linear(hidden_dim, attention_dim)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.lstm = nn.LSTM(attention_dim + mel_dim, hidden_dim, 2, batch_first=True)\n",
    "        self.mel_projection = nn.Linear(hidden_dim, mel_dim)\n",
    "        self.stop_projection = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, encoder_outputs, mel_targets=None, max_length=1000):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        \n",
    "        if self.training and mel_targets is not None:\n",
    "            return self.teacher_forcing(encoder_outputs, mel_targets)\n",
    "        else:\n",
    "            return self.inference(encoder_outputs, max_length)\n",
    "    \n",
    "    def teacher_forcing(self, encoder_outputs, mel_targets):\n",
    "        \"\"\"Training with teacher forcing\"\"\"\n",
    "        batch_size, max_mel_len, mel_dim = mel_targets.shape\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        \n",
    "        outputs = []\n",
    "        stop_tokens = []\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h_0 = torch.zeros(2, batch_size, self.hidden_dim).to(encoder_outputs.device)\n",
    "        c_0 = torch.zeros(2, batch_size, self.hidden_dim).to(encoder_outputs.device)\n",
    "        hidden = (h_0, c_0)\n",
    "        \n",
    "        # Start with zero frame\n",
    "        prev_mel = torch.zeros(batch_size, 1, mel_dim).to(encoder_outputs.device)\n",
    "        \n",
    "        for t in range(max_mel_len):\n",
    "            # Attention\n",
    "            attention_weights = self.compute_attention(\n",
    "                encoder_outputs, hidden[0][-1].unsqueeze(1)\n",
    "            )\n",
    "            context = torch.bmm(attention_weights, encoder_outputs)\n",
    "            \n",
    "            # Decoder input\n",
    "            decoder_input = torch.cat([context, prev_mel], dim=-1)\n",
    "            \n",
    "            # LSTM\n",
    "            lstm_out, hidden = self.lstm(decoder_input, hidden)\n",
    "            lstm_out = self.dropout(lstm_out)\n",
    "            \n",
    "            # Predictions\n",
    "            mel_pred = self.mel_projection(lstm_out)\n",
    "            stop_pred = self.stop_projection(lstm_out)\n",
    "            \n",
    "            outputs.append(mel_pred)\n",
    "            stop_tokens.append(stop_pred)\n",
    "            \n",
    "            # Use ground truth for next input (teacher forcing)\n",
    "            if t < max_mel_len - 1:\n",
    "                prev_mel = mel_targets[:, t:t+1, :]\n",
    "        \n",
    "        mel_outputs = torch.cat(outputs, dim=1)\n",
    "        stop_outputs = torch.cat(stop_tokens, dim=1)\n",
    "        \n",
    "        return mel_outputs, stop_outputs\n",
    "    \n",
    "    def inference(self, encoder_outputs, max_length):\n",
    "        \"\"\"Inference without teacher forcing\"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        \n",
    "        outputs = []\n",
    "        stop_tokens = []\n",
    "        \n",
    "        # Initialize\n",
    "        h_0 = torch.zeros(2, batch_size, self.hidden_dim).to(encoder_outputs.device)\n",
    "        c_0 = torch.zeros(2, batch_size, self.hidden_dim).to(encoder_outputs.device)\n",
    "        hidden = (h_0, c_0)\n",
    "        \n",
    "        prev_mel = torch.zeros(batch_size, 1, self.mel_dim).to(encoder_outputs.device)\n",
    "        \n",
    "        for t in range(max_length):\n",
    "            # Attention\n",
    "            attention_weights = self.compute_attention(\n",
    "                encoder_outputs, hidden[0][-1].unsqueeze(1)\n",
    "            )\n",
    "            context = torch.bmm(attention_weights, encoder_outputs)\n",
    "            \n",
    "            # Decoder input\n",
    "            decoder_input = torch.cat([context, prev_mel], dim=-1)\n",
    "            \n",
    "            # LSTM\n",
    "            lstm_out, hidden = self.lstm(decoder_input, hidden)\n",
    "            \n",
    "            # Predictions\n",
    "            mel_pred = self.mel_projection(lstm_out)\n",
    "            stop_pred = self.stop_projection(lstm_out)\n",
    "            \n",
    "            outputs.append(mel_pred)\n",
    "            stop_tokens.append(stop_pred)\n",
    "            \n",
    "            # Use prediction for next input\n",
    "            prev_mel = mel_pred\n",
    "            \n",
    "            # Check if we should stop\n",
    "            if torch.sigmoid(stop_pred).item() > 0.5:\n",
    "                break\n",
    "        \n",
    "        mel_outputs = torch.cat(outputs, dim=1)\n",
    "        stop_outputs = torch.cat(stop_tokens, dim=1)\n",
    "        \n",
    "        return mel_outputs, stop_outputs\n",
    "    \n",
    "    def compute_attention(self, encoder_outputs, decoder_hidden):\n",
    "        \"\"\"Compute attention weights\"\"\"\n",
    "        # encoder_outputs: (batch, seq_len, hidden_dim)\n",
    "        # decoder_hidden: (batch, 1, hidden_dim)\n",
    "        \n",
    "        query = self.attention_query(decoder_hidden)  # (batch, 1, attention_dim)\n",
    "        key = self.attention_key(encoder_outputs)     # (batch, seq_len, attention_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.bmm(query, key.transpose(1, 2))  # (batch, 1, seq_len)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        return attention_weights\n",
    "\n",
    "class PashtoTTSModel(nn.Module):\n",
    "    \"\"\"Complete TTS model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, mel_dim=80, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = TTSEncoder(vocab_size, hidden_dim=hidden_dim)\n",
    "        self.decoder = TTSDecoder(hidden_dim=hidden_dim, mel_dim=mel_dim)\n",
    "        \n",
    "    def forward(self, text_sequences, text_lengths, mel_targets=None):\n",
    "        # Encode text\n",
    "        encoder_outputs = self.encoder(text_sequences, text_lengths)\n",
    "        \n",
    "        # Decode to mel spectrograms\n",
    "        mel_outputs, stop_outputs = self.decoder(encoder_outputs, mel_targets)\n",
    "        \n",
    "        return mel_outputs, stop_outputs\n",
    "\n",
    "class TTSTrainer:\n",
    "    \"\"\"Training class for TTS model\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, lr=1e-3, mel_loss_weight=1.0, stop_loss_weight=0.5):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.mel_loss_weight = mel_loss_weight\n",
    "        self.stop_loss_weight = stop_loss_weight\n",
    "        \n",
    "        # Optimizers\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.5)\n",
    "        \n",
    "        # Loss functions\n",
    "        self.mel_criterion = nn.MSELoss()\n",
    "        self.stop_criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_mel_loss = 0\n",
    "        total_stop_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Move to device\n",
    "            text_sequences = batch['text_sequences'].to(self.device)\n",
    "            text_lengths = batch['text_lengths'].to(self.device)\n",
    "            mel_targets = batch['mel_spectrograms'].to(self.device)\n",
    "            mel_lengths = batch['mel_lengths'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            mel_outputs, stop_outputs = self.model(text_sequences, text_lengths, mel_targets)\n",
    "            \n",
    "            # Compute losses\n",
    "            mel_loss = self.compute_mel_loss(mel_outputs, mel_targets, mel_lengths)\n",
    "            stop_loss = self.compute_stop_loss(stop_outputs, mel_lengths)\n",
    "            \n",
    "            loss = self.mel_loss_weight * mel_loss + self.stop_loss_weight * stop_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            total_loss += loss.item()\n",
    "            total_mel_loss += mel_loss.item()\n",
    "            total_stop_loss += stop_loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Batch {batch_idx}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.4f}, '\n",
    "                      f'Mel: {mel_loss.item():.4f}, '\n",
    "                      f'Stop: {stop_loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_mel_loss = total_mel_loss / len(train_loader)\n",
    "        avg_stop_loss = total_stop_loss / len(train_loader)\n",
    "        \n",
    "        return avg_loss, avg_mel_loss, avg_stop_loss\n",
    "    \n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"Validate model\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_mel_loss = 0\n",
    "        total_stop_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                text_sequences = batch['text_sequences'].to(self.device)\n",
    "                text_lengths = batch['text_lengths'].to(self.device)\n",
    "                mel_targets = batch['mel_spectrograms'].to(self.device)\n",
    "                mel_lengths = batch['mel_lengths'].to(self.device)\n",
    "                \n",
    "                mel_outputs, stop_outputs = self.model(text_sequences, text_lengths, mel_targets)\n",
    "                \n",
    "                mel_loss = self.compute_mel_loss(mel_outputs, mel_targets, mel_lengths)\n",
    "                stop_loss = self.compute_stop_loss(stop_outputs, mel_lengths)\n",
    "                \n",
    "                loss = self.mel_loss_weight * mel_loss + self.stop_loss_weight * stop_loss\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_mel_loss += mel_loss.item()\n",
    "                total_stop_loss += stop_loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        avg_mel_loss = total_mel_loss / len(val_loader)\n",
    "        avg_stop_loss = total_stop_loss / len(val_loader)\n",
    "        \n",
    "        return avg_loss, avg_mel_loss, avg_stop_loss\n",
    "    \n",
    "    def compute_mel_loss(self, outputs, targets, lengths):\n",
    "        \"\"\"Compute mel spectrogram loss\"\"\"\n",
    "        loss = 0\n",
    "        for i, length in enumerate(lengths):\n",
    "            loss += self.mel_criterion(outputs[i, :length], targets[i, :length])\n",
    "        return loss / len(lengths)\n",
    "    \n",
    "    def compute_stop_loss(self, outputs, lengths):\n",
    "        \"\"\"Compute stop token loss\"\"\"\n",
    "        batch_size = outputs.size(0)\n",
    "        max_len = outputs.size(1)\n",
    "        \n",
    "        # Create stop token targets\n",
    "        stop_targets = torch.zeros_like(outputs)\n",
    "        for i, length in enumerate(lengths):\n",
    "            if length < max_len:\n",
    "                stop_targets[i, length-1] = 1.0  # Stop at the end\n",
    "        \n",
    "        return self.stop_criterion(outputs, stop_targets)\n",
    "    \n",
    "    def save_checkpoint(self, epoch, loss, path):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'loss': loss,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses\n",
    "        }\n",
    "        torch.save(checkpoint, path)\n",
    "        print(f\"Checkpoint saved: {path}\")\n",
    "    \n",
    "    def load_checkpoint(self, path):\n",
    "        \"\"\"Load model checkpoint\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.train_losses = checkpoint.get('train_losses', [])\n",
    "        self.val_losses = checkpoint.get('val_losses', [])\n",
    "        print(f\"Checkpoint loaded: {path}\")\n",
    "        return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    JSON_PATH = r\"C:\\Users\\PC\\Desktop\\scirpts\\json\\new6.json\"\n",
    "    AUDIO_PATH = r\"C:\\Users\\PC\\Downloads\\AudioFiles\"\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 1e-3\n",
    "    NUM_EPOCHS = 100\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    # Initialize processors\n",
    "    text_processor = PashtoTextProcessor()\n",
    "    audio_processor = AudioProcessor(sample_rate=22050, n_mels=80)\n",
    "    \n",
    "    print(f\"Vocabulary size: {text_processor.vocab_size}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = PashtoTTSDataset(JSON_PATH, AUDIO_PATH, text_processor, audio_processor)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(dataset)), test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "        collate_fn=collate_fn, num_workers=2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "        collate_fn=collate_fn, num_workers=2\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PashtoTTSModel(\n",
    "        vocab_size=text_processor.vocab_size,\n",
    "        mel_dim=audio_processor.n_mels,\n",
    "        hidden_dim=512\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = TTSTrainer(model, DEVICE, lr=LEARNING_RATE)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(\"tts_checkpoints\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_mel_loss, train_stop_loss = trainer.train_epoch(train_loader)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_mel_loss, val_stop_loss = trainer.validate(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        trainer.scheduler.step()\n",
    "        \n",
    "        # Store losses\n",
    "        trainer.train_losses.append(train_loss)\n",
    "        trainer.val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} (Mel: {train_mel_loss:.4f}, Stop: {train_stop_loss:.4f})\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} (Mel: {val_mel_loss:.4f}, Stop: {val_stop_loss:.4f})\")\n",
    "        print(f\"Learning Rate: {trainer.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trainer.save_checkpoint(\n",
    "                epoch, val_loss, output_dir / \"best_model.pth\"\n",
    "            )\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            trainer.save_checkpoint(\n",
    "                epoch, val_loss, output_dir / f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "            )\n",
    "        \n",
    "        # Plot losses\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(trainer.train_losses, label='Train Loss')\n",
    "            plt.plot(trainer.val_losses, label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training Progress')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(output_dir / 'training_progress.png')\n",
    "            plt.close()\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # Save final model and processors\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'text_processor': text_processor,\n",
    "        'audio_processor': audio_processor,\n",
    "        'model_config': {\n",
    "            'vocab_size': text_processor.vocab_size,\n",
    "            'mel_dim': audio_processor.n_mels,\n",
    "            'hidden_dim': 512\n",
    "        }\n",
    "    }, output_dir / 'final_model.pth')\n",
    "    \n",
    "    print(f\"Final model saved to: {output_dir / 'final_model.pth'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Function to test the trained model\n",
    "def test_model(model_path, text_input):\n",
    "    \"\"\"Test the trained model with sample text\"\"\"\n",
    "    \n",
    "    # Load model and processors\n",
    "    checkpoint = torch.load(model_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    text_processor = checkpoint['text_processor']\n",
    "    audio_processor = checkpoint['audio_processor']\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = PashtoTTSModel(**checkpoint['model_config']).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Process input text\n",
    "    text_sequence = text_processor.text_to_sequence(text_input)\n",
    "    text_tensor = torch.tensor([text_sequence], dtype=torch.long).to(device)\n",
    "    text_lengths = torch.tensor([len(text_sequence)]).to(device)\n",
    "    \n",
    "    # Generate mel spectrogram\n",
    "    with torch.no_grad():\n",
    "        mel_outputs, stop_outputs = model(text_tensor, text_lengths)\n",
    "    \n",
    "    # Convert mel spectrogram back to audio (you'll need a vocoder for this)\n",
    "    mel_spec = mel_outputs.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    print(f\"Generated mel spectrogram shape: {mel_spec.shape}\")\n",
    "    print(f\"Input text: {text_input}\")\n",
    "    \n",
    "    return mel_spec\n",
    "\n",
    "# Additional utility functions for inference and vocoder integration\n",
    "\n",
    "class MelToAudioConverter:\n",
    "    \"\"\"Convert mel spectrograms back to audio using Griffin-Lim algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=22050, n_fft=1024, hop_length=256, win_length=1024, n_iter=60):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "        # Create inverse mel transform\n",
    "        self.mel_scale = torchaudio.transforms.MelScale(\n",
    "            n_mels=80, sample_rate=sample_rate, n_stft=n_fft // 2 + 1\n",
    "        )\n",
    "        self.inverse_mel_scale = torchaudio.transforms.InverseMelScale(\n",
    "            n_stft=n_fft // 2 + 1, n_mels=80, sample_rate=sample_rate\n",
    "        )\n",
    "        self.griffin_lim = torchaudio.transforms.GriffinLim(\n",
    "            n_fft=n_fft, hop_length=hop_length, win_length=win_length, n_iter=n_iter\n",
    "        )\n",
    "    \n",
    "    def mel_to_audio(self, mel_spectrogram):\n",
    "        \"\"\"Convert mel spectrogram to audio waveform\"\"\"\n",
    "        # Convert from log scale\n",
    "        mel_spec = torch.exp(torch.tensor(mel_spectrogram).T)  # (n_mels, time)\n",
    "        \n",
    "        # Convert mel to linear spectrogram\n",
    "        linear_spec = self.inverse_mel_scale(mel_spec)\n",
    "        \n",
    "        # Convert to audio using Griffin-Lim\n",
    "        waveform = self.griffin_lim(linear_spec)\n",
    "        \n",
    "        return waveform.numpy()\n",
    "\n",
    "def create_inference_pipeline():\n",
    "    \"\"\"Create a complete inference pipeline\"\"\"\n",
    "    \n",
    "    class PashtoTTSInference:\n",
    "        def __init__(self, model_path):\n",
    "            # Load model and processors\n",
    "            checkpoint = torch.load(model_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            self.text_processor = checkpoint['text_processor']\n",
    "            self.audio_processor = checkpoint['audio_processor']\n",
    "            self.mel_converter = MelToAudioConverter()\n",
    "            \n",
    "            # Initialize model\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            self.model = PashtoTTSModel(**checkpoint['model_config']).to(self.device)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.model.eval()\n",
    "            \n",
    "            print(\"TTS model loaded successfully!\")\n",
    "        \n",
    "        def synthesize(self, text, output_path=None):\n",
    "            \"\"\"Convert text to speech\"\"\"\n",
    "            # Process text\n",
    "            text_sequence = self.text_processor.text_to_sequence(text)\n",
    "            text_tensor = torch.tensor([text_sequence], dtype=torch.long).to(self.device)\n",
    "            text_lengths = torch.tensor([len(text_sequence)]).to(self.device)\n",
    "            \n",
    "            # Generate mel spectrogram\n",
    "            with torch.no_grad():\n",
    "                mel_outputs, stop_outputs = self.model(text_tensor, text_lengths)\n",
    "            \n",
    "            # Convert to audio\n",
    "            mel_spec = mel_outputs.squeeze(0).cpu().numpy()\n",
    "            audio = self.mel_converter.mel_to_audio(mel_spec)\n",
    "            \n",
    "            # Save audio if path provided\n",
    "            if output_path:\n",
    "                torchaudio.save(output_path, torch.tensor(audio).unsqueeze(0), self.audio_processor.sample_rate)\n",
    "                print(f\"Audio saved to: {output_path}\")\n",
    "            \n",
    "            return audio, mel_spec\n",
    "        \n",
    "        def batch_synthesize(self, texts, output_dir):\n",
    "            \"\"\"Synthesize multiple texts\"\"\"\n",
    "            Path(output_dir).mkdir(exist_ok=True)\n",
    "            \n",
    "            for i, text in enumerate(texts):\n",
    "                output_path = Path(output_dir) / f\"synthesis_{i+1}.wav\"\n",
    "                audio, mel_spec = self.synthesize(text, output_path)\n",
    "                print(f\"Synthesized: {text[:50]}...\")\n",
    "    \n",
    "    return PashtoTTSInference\n",
    "\n",
    "# Advanced training features\n",
    "\n",
    "class AdvancedTTSTrainer(TTSTrainer):\n",
    "    \"\"\"Enhanced trainer with additional features\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, lr=1e-3, mel_loss_weight=1.0, stop_loss_weight=0.5):\n",
    "        super().__init__(model, device, lr, mel_loss_weight, stop_loss_weight)\n",
    "        \n",
    "        # Add attention visualization\n",
    "        self.attention_plots = []\n",
    "        \n",
    "        # Add mixed precision training\n",
    "        self.scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "        \n",
    "        # Add early stopping\n",
    "        self.patience = 15\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "    \n",
    "    def train_epoch_mixed_precision(self, train_loader):\n",
    "        \"\"\"Training with mixed precision for faster training\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            text_sequences = batch['text_sequences'].to(self.device)\n",
    "            text_lengths = batch['text_lengths'].to(self.device)\n",
    "            mel_targets = batch['mel_spectrograms'].to(self.device)\n",
    "            mel_lengths = batch['mel_lengths'].to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.scaler:\n",
    "                # Mixed precision forward pass\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    mel_outputs, stop_outputs = self.model(text_sequences, text_lengths, mel_targets)\n",
    "                    mel_loss = self.compute_mel_loss(mel_outputs, mel_targets, mel_lengths)\n",
    "                    stop_loss = self.compute_stop_loss(stop_outputs, mel_lengths)\n",
    "                    loss = self.mel_loss_weight * mel_loss + self.stop_loss_weight * stop_loss\n",
    "                \n",
    "                # Backward pass with scaling\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                # Regular forward pass\n",
    "                mel_outputs, stop_outputs = self.model(text_sequences, text_lengths, mel_targets)\n",
    "                mel_loss = self.compute_mel_loss(mel_outputs, mel_targets, mel_lengths)\n",
    "                stop_loss = self.compute_stop_loss(stop_outputs, mel_lengths)\n",
    "                loss = self.mel_loss_weight * mel_loss + self.stop_loss_weight * stop_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    def check_early_stopping(self, val_loss):\n",
    "        \"\"\"Check if training should stop early\"\"\"\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.patience_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            if self.patience_counter >= self.patience:\n",
    "                print(f\"Early stopping triggered after {self.patience} epochs without improvement\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Data augmentation for better training\n",
    "\n",
    "class TTSDataAugmenter:\n",
    "    \"\"\"Data augmentation techniques for TTS training\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=22050):\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def add_noise(self, waveform, noise_factor=0.005):\n",
    "        \"\"\"Add random noise to audio\"\"\"\n",
    "        noise = torch.randn_like(waveform) * noise_factor\n",
    "        return waveform + noise\n",
    "    \n",
    "    def time_stretch(self, waveform, stretch_factor=None):\n",
    "        \"\"\"Time stretching (speed change without pitch change)\"\"\"\n",
    "        if stretch_factor is None:\n",
    "            stretch_factor = np.random.uniform(0.9, 1.1)\n",
    "        \n",
    "        # Simple time stretching using interpolation\n",
    "        original_length = waveform.shape[-1]\n",
    "        new_length = int(original_length / stretch_factor)\n",
    "        \n",
    "        indices = torch.linspace(0, original_length - 1, new_length)\n",
    "        stretched = torch.nn.functional.interpolate(\n",
    "            waveform.unsqueeze(0).unsqueeze(0), \n",
    "            size=new_length, \n",
    "            mode='linear', \n",
    "            align_corners=True\n",
    "        ).squeeze()\n",
    "        \n",
    "        return stretched\n",
    "    \n",
    "    def pitch_shift(self, waveform, n_steps=None):\n",
    "        \"\"\"Pitch shifting using phase vocoder\"\"\"\n",
    "        if n_steps is None:\n",
    "            n_steps = np.random.uniform(-2, 2)  # ±2 semitones\n",
    "        \n",
    "        # Convert to numpy for librosa processing\n",
    "        audio_np = waveform.numpy()\n",
    "        shifted = librosa.effects.pitch_shift(audio_np, sr=self.sample_rate, n_steps=n_steps)\n",
    "        return torch.tensor(shifted)\n",
    "\n",
    "# Model evaluation metrics\n",
    "\n",
    "class TTSEvaluator:\n",
    "    \"\"\"Evaluation metrics for TTS model\"\"\"\n",
    "    \n",
    "    def __init__(self, model, text_processor, audio_processor, device):\n",
    "        self.model = model\n",
    "        self.text_processor = text_processor\n",
    "        self.audio_processor = audio_processor\n",
    "        self.device = device\n",
    "    \n",
    "    def compute_mel_distance(self, predicted_mel, target_mel):\n",
    "        \"\"\"Compute mel spectrogram distance\"\"\"\n",
    "        mse = torch.nn.functional.mse_loss(predicted_mel, target_mel)\n",
    "        return mse.item()\n",
    "    \n",
    "    def evaluate_dataset(self, dataset, num_samples=None):\n",
    "        \"\"\"Evaluate model on a dataset\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        if num_samples:\n",
    "            indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "        else:\n",
    "            indices = range(len(dataset))\n",
    "        \n",
    "        total_mel_distance = 0\n",
    "        valid_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx in indices:\n",
    "                sample = dataset[idx]\n",
    "                \n",
    "                # Prepare inputs\n",
    "                text_seq = sample['text_sequence'].unsqueeze(0).to(self.device)\n",
    "                text_len = torch.tensor([len(sample['text_sequence'])]).to(self.device)\n",
    "                target_mel = sample['mel_spectrogram'].to(self.device)\n",
    "                \n",
    "                # Generate prediction\n",
    "                pred_mel, _ = self.model(text_seq, text_len)\n",
    "                pred_mel = pred_mel.squeeze(0)\n",
    "                \n",
    "                # Compute distance\n",
    "                min_len = min(pred_mel.shape[0], target_mel.shape[0])\n",
    "                distance = self.compute_mel_distance(\n",
    "                    pred_mel[:min_len], target_mel[:min_len]\n",
    "                )\n",
    "                \n",
    "                total_mel_distance += distance\n",
    "                valid_samples += 1\n",
    "        \n",
    "        avg_distance = total_mel_distance / valid_samples if valid_samples > 0 else float('inf')\n",
    "        return avg_distance\n",
    "\n",
    "# Usage example and testing\n",
    "\n",
    "def run_complete_training():\n",
    "    \"\"\"Complete training pipeline with all features\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    config = {\n",
    "        'json_path': r\"C:\\Users\\PC\\Desktop\\scirpts\\json\\new6.json\",\n",
    "        'audio_path': r\"C:\\Users\\PC\\Downloads\\AudioFiles\",\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 1e-3,\n",
    "        'num_epochs': 50,\n",
    "        'hidden_dim': 512,\n",
    "        'mel_dim': 80,\n",
    "        'sample_rate': 22050,\n",
    "        'use_mixed_precision': True,\n",
    "        'early_stopping': True\n",
    "    }\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on: {device}\")\n",
    "    \n",
    "    # Initialize components\n",
    "    text_processor = PashtoTextProcessor()\n",
    "    audio_processor = AudioProcessor(\n",
    "        sample_rate=config['sample_rate'],\n",
    "        n_mels=config['mel_dim']\n",
    "    )\n",
    "    \n",
    "    # Create dataset with augmentation\n",
    "    dataset = PashtoTTSDataset(\n",
    "        config['json_path'], \n",
    "        config['audio_path'], \n",
    "        text_processor, \n",
    "        audio_processor\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = int(0.1 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0  # Set to 0 for Windows compatibility\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PashtoTTSModel(\n",
    "        vocab_size=text_processor.vocab_size,\n",
    "        mel_dim=config['mel_dim'],\n",
    "        hidden_dim=config['hidden_dim']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = AdvancedTTSTrainer(model, device, lr=config['learning_rate'])\n",
    "    \n",
    "    # Training loop\n",
    "    output_dir = Path(\"pashto_tts_output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
    "        \n",
    "        # Train\n",
    "        if config['use_mixed_precision']:\n",
    "            train_loss = trainer.train_epoch_mixed_precision(train_loader)\n",
    "        else:\n",
    "            train_loss, _, _ = trainer.train_epoch(train_loader)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, _, _ = trainer.validate(val_loader)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            trainer.save_checkpoint(\n",
    "                epoch, val_loss, output_dir / f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "            )\n",
    "        \n",
    "        # Early stopping\n",
    "        if config['early_stopping'] and trainer.check_early_stopping(val_loss):\n",
    "            break\n",
    "        \n",
    "        trainer.scheduler.step()\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = output_dir / \"final_pashto_tts_model.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'text_processor': text_processor,\n",
    "        'audio_processor': audio_processor,\n",
    "        'model_config': {\n",
    "            'vocab_size': text_processor.vocab_size,\n",
    "            'mel_dim': config['mel_dim'],\n",
    "            'hidden_dim': config['hidden_dim']\n",
    "        },\n",
    "        'config': config\n",
    "    }, final_model_path)\n",
    "    \n",
    "    print(f\"Training completed! Model saved to: {final_model_path}\")\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"\\nTesting model...\")\n",
    "    inference_pipeline = create_inference_pipeline()\n",
    "    tts_model = inference_pipeline(final_model_path)\n",
    "    \n",
    "    # Test with sample Pashto text\n",
    "    test_texts = [\n",
    "        \"انګېزه د شیانو علت، سبب او رېښې ته وایي.\",\n",
    "        \"افغانان د انیس په نوم مجله خپروي.\",\n",
    "        \"اورکی د اور د بلېدو لامل کېږي.\"\n",
    "    ]\n",
    "    \n",
    "    test_output_dir = output_dir / \"test_outputs\"\n",
    "    tts_model.batch_synthesize(test_texts, test_output_dir)\n",
    "    \n",
    "    print(\"Testing completed!\")\n",
    "\n",
    "# Quick start function\n",
    "def quick_start():\n",
    "    \"\"\"Quick start with minimal configuration\"\"\"\n",
    "    print(\"Starting Pashto TTS training...\")\n",
    "    print(\"Make sure your paths are correct:\")\n",
    "    print(\"JSON: C:\\\\Users\\\\PC\\\\Desktop\\\\scirpts\\\\json\\\\new6.json\")\n",
    "    print(\"Audio: C:\\\\Users\\\\PC\\\\Downloads\\\\AudioFiles\")\n",
    "    \n",
    "    try:\n",
    "        run_complete_training()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        print(\"Check your file paths and make sure audio files exist!\")\n",
    "\n",
    "# Run the training\n",
    "if __name__ == \"__main__\":\n",
    "    quick_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch RTX 5060 Ti",
   "language": "python",
   "name": "pytorch_rtx5060"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
