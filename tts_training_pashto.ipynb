{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94c46e44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "Initializing phoneme tokenizer...\n",
      "Test: ښه راغلاست -> torch.Size([1, 150])\n",
      "Loading SpeechT5 model...\n",
      "Original vocab size: 81\n",
      "New vocab size: 49\n",
      "Moving model to GPU...\n",
      "❌ GPU move failed: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 197\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMoving model to GPU...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda:0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m    199\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Model on GPU\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch_build\\Lib\\site-packages\\transformers\\modeling_utils.py:3953\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3948\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3949\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3950\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3951\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3952\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3953\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch_build\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1367\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1364\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1365\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch_build\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    926\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    930\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    937\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch_build\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    926\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    930\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    937\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 927 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch_build\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    926\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    930\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    937\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch_build\\Lib\\site-packages\\torch\\nn\\modules\\module.py:954\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    955\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    957\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch_build\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1353\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1346\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1347\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1348\u001b[39m             device,\n\u001b[32m   1349\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1350\u001b[39m             non_blocking,\n\u001b[32m   1351\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1352\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1359\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, TrainingArguments, Trainer\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# CRITICAL: Ensure CUDA is working from previous cell\n",
    "assert torch.cuda.is_available(), \"CUDA must be available! Run the CUDA fix cell first.\"\n",
    "assert torch.cuda.device_count() > 0, \"No CUDA devices found!\"\n",
    "\n",
    "print(f\"✅ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Memory management\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Pashto Phoneme Tokenizer (same as before, but with CUDA-safe methods)\n",
    "class PashtoPhonemeTokenizer:\n",
    "    \"\"\"CUDA-safe Pashto phoneme tokenizer\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.phonemes = {\n",
    "            '<PAD>': '<PAD>', '<UNK>': '<UNK>', '<BOS>': '<BOS>', '<EOS>': '<EOS>', \n",
    "            '<SIL>': '<SIL>', '<WB>': '<WB>',\n",
    "            'a': 'a', 'aː': 'aː', 'e': 'e', 'eː': 'eː', 'i': 'i', 'iː': 'iː',\n",
    "            'o': 'o', 'oː': 'oː', 'u': 'u', 'uː': 'uː', 'ə': 'ə',\n",
    "            'p': 'p', 'b': 'b', 't': 't', 'd': 'd', 'ʈ': 'ʈ', 'ɖ': 'ɖ',\n",
    "            'k': 'k', 'g': 'g', 'q': 'q', 'ʔ': 'ʔ', 'f': 'f', 'v': 'v',\n",
    "            's': 's', 'z': 'z', 'ʃ': 'ʃ', 'ʒ': 'ʒ', 'ʂ': 'ʂ', 'ʐ': 'ʐ',\n",
    "            'x': 'x', 'ɣ': 'ɣ', 'h': 'h', 'ʣ': 'ʣ', 'ʦ': 'ʦ', 'm': 'm',\n",
    "            'n': 'n', 'ɳ': 'ɳ', 'ŋ': 'ŋ', 'l': 'l', 'r': 'r', 'ɽ': 'ɽ',\n",
    "            'j': 'j', 'w': 'w',\n",
    "        }\n",
    "        \n",
    "        self.consonant_map = {\n",
    "            'پ': 'p', 'ب': 'b', 'ت': 't', 'د': 'd', 'ټ': 'ʈ', 'ډ': 'ɖ',\n",
    "            'ک': 'k', 'ګ': 'g', 'ق': 'q', 'ع': 'ʔ', 'ف': 'f', 'س': 's',\n",
    "            'ز': 'z', 'ش': 'ʃ', 'ژ': 'ʒ', 'ښ': 'ʂ', 'ږ': 'ʐ', 'خ': 'x',\n",
    "            'غ': 'ɣ', 'ح': 'h', 'ه': 'h', 'ځ': 'ʣ', 'څ': 'ʦ', 'م': 'm',\n",
    "            'ن': 'n', 'ڼ': 'ɳ', 'ل': 'l', 'ر': 'r', 'ړ': 'ɽ', 'ي': 'j', 'و': 'w'\n",
    "        }\n",
    "        \n",
    "        self.vowel_map = {\n",
    "            'ا': 'a', 'آ': 'aː', 'ې': 'e', 'ي': 'i', 'و': 'u', 'ه': 'ə'\n",
    "        }\n",
    "        \n",
    "        self.phoneme_to_id = {p: i for i, p in enumerate(self.phonemes.keys())}\n",
    "        self.id_to_phoneme = {i: p for p, i in self.phoneme_to_id.items()}\n",
    "        self.vocab_size = len(self.phonemes)\n",
    "        \n",
    "        self.pad_token_id = self.phoneme_to_id['<PAD>']\n",
    "        self.unk_token_id = self.phoneme_to_id['<UNK>']\n",
    "        self.bos_token_id = self.phoneme_to_id['<BOS>']\n",
    "        self.eos_token_id = self.phoneme_to_id['<EOS>']\n",
    "        \n",
    "        self.model_input_names = ['input_ids']\n",
    "        self.model_max_length = 150  # Shorter sequences for stability\n",
    "    \n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        text = re.sub(r'[\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]', '', text)\n",
    "        text = re.sub(r'[\\u200c\\u200d\\u200e\\u200f]', '', text)\n",
    "        text = text.replace('،', ' ').replace('؟', ' ').replace('؛', ' ').replace('!', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def text_to_phonemes(self, text: str) -> List[str]:\n",
    "        text = self.normalize_text(text)\n",
    "        words = text.split()\n",
    "        phonemes = ['<BOS>']\n",
    "        \n",
    "        for word_idx, word in enumerate(words):\n",
    "            if word_idx > 0:\n",
    "                phonemes.append('<WB>')\n",
    "            phonemes.extend(self.word_to_phonemes(word))\n",
    "        \n",
    "        phonemes.append('<EOS>')\n",
    "        return phonemes[:self.model_max_length]  # Truncate long sequences\n",
    "    \n",
    "    def word_to_phonemes(self, word: str) -> List[str]:\n",
    "        phonemes = []\n",
    "        for char in word:\n",
    "            if char in self.consonant_map:\n",
    "                phonemes.append(self.consonant_map[char])\n",
    "            elif char in self.vowel_map:\n",
    "                phonemes.append(self.vowel_map[char])\n",
    "            elif char == 'و':\n",
    "                phonemes.append('w')  # Simplified waw handling\n",
    "            elif char.strip():\n",
    "                phonemes.append('<UNK>')\n",
    "        return phonemes\n",
    "    \n",
    "    def __call__(self, texts, padding=True, truncation=True, max_length=None, return_tensors=\"pt\"):\n",
    "        if max_length is None:\n",
    "            max_length = self.model_max_length\n",
    "        \n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_input_ids = []\n",
    "        all_attention_masks = []\n",
    "        \n",
    "        for text in texts:\n",
    "            phonemes = self.text_to_phonemes(text)\n",
    "            input_ids = [self.phoneme_to_id.get(p, self.unk_token_id) for p in phonemes]\n",
    "            \n",
    "            # Ensure all token IDs are valid\n",
    "            input_ids = [min(id, self.vocab_size - 1) for id in input_ids]\n",
    "            \n",
    "            if truncation and len(input_ids) > max_length:\n",
    "                input_ids = input_ids[:max_length-1] + [self.eos_token_id]\n",
    "            \n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            \n",
    "            if padding and len(input_ids) < max_length:\n",
    "                padding_length = max_length - len(input_ids)\n",
    "                input_ids.extend([self.pad_token_id] * padding_length)\n",
    "                attention_mask.extend([0] * padding_length)\n",
    "            \n",
    "            all_input_ids.append(input_ids)\n",
    "            all_attention_masks.append(attention_mask)\n",
    "        \n",
    "        result = {\n",
    "            'input_ids': all_input_ids,\n",
    "            'attention_mask': all_attention_masks\n",
    "        }\n",
    "        \n",
    "        if return_tensors == \"pt\":\n",
    "            # Create tensors on CPU first, then move to GPU in training\n",
    "            result = {k: torch.tensor(v, dtype=torch.long) for k, v in result.items()}\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.phoneme_to_id.copy()\n",
    "\n",
    "# Custom processor\n",
    "class CUDASafeSpeechT5Processor:\n",
    "    def __init__(self, phoneme_tokenizer):\n",
    "        self.original_processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "        self.tokenizer = phoneme_tokenizer\n",
    "        self.feature_extractor = self.original_processor.feature_extractor\n",
    "    \n",
    "    def save_pretrained(self, save_directory):\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        self.feature_extractor.save_pretrained(save_directory)\n",
    "        vocab_file = os.path.join(save_directory, \"phoneme_vocab.json\")\n",
    "        with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.tokenizer.get_vocab(), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Initialize components\n",
    "print(\"Initializing phoneme tokenizer...\")\n",
    "phoneme_tokenizer = PashtoPhonemeTokenizer()\n",
    "processor = CUDASafeSpeechT5Processor(phoneme_tokenizer)\n",
    "\n",
    "# Test tokenizer\n",
    "test_text = \"ښه راغلاست\"\n",
    "test_result = phoneme_tokenizer(test_text)\n",
    "print(f\"Test: {test_text} -> {test_result['input_ids'].shape}\")\n",
    "\n",
    "# Setup paths\n",
    "PASHTO_DATA_JSON = r\"C:\\Users\\PC\\Music\\jj\\new6.json\"\n",
    "LOCAL_AUDIO_DIR = r\"C:\\Users\\PC\\Downloads\\AudioFiles\"\n",
    "OUTPUT_DIR = \"s2t5_pashto_cuda_tts\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load model with CUDA safety\n",
    "print(\"Loading SpeechT5 model...\")\n",
    "\n",
    "# Load on CPU first to avoid CUDA issues during initialization\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\n",
    "    \"microsoft/speecht5_tts\", \n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=None  # Load on CPU first\n",
    ")\n",
    "\n",
    "print(f\"Original vocab size: {model.config.vocab_size}\")\n",
    "print(f\"New vocab size: {phoneme_tokenizer.vocab_size}\")\n",
    "\n",
    "# Resize embeddings\n",
    "model.resize_token_embeddings(phoneme_tokenizer.vocab_size)\n",
    "\n",
    "# Update config\n",
    "model.config.vocab_size = phoneme_tokenizer.vocab_size\n",
    "model.config.pad_token_id = phoneme_tokenizer.pad_token_id\n",
    "model.config.bos_token_id = phoneme_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = phoneme_tokenizer.eos_token_id\n",
    "\n",
    "# Move to GPU safely\n",
    "print(\"Moving model to GPU...\")\n",
    "try:\n",
    "    model = model.to('cuda:0')\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✅ Model on GPU\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ GPU move failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Dataset loading (same as before but smaller)\n",
    "def load_pashto_dataset(json_file_path: str, max_samples: int = 50):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if max_samples and len(data) > max_samples:\n",
    "        random.seed(42)\n",
    "        data = random.sample(data, max_samples)\n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        'audio_url': [item['file_url'] for item in data],\n",
    "        'text': [item['sentence'] for item in data],\n",
    "        'speaker_id': [f\"{item['gender']}_{item['accent']}\" for item in data]\n",
    "    })\n",
    "\n",
    "def load_local_audio(example, idx=None):\n",
    "    filename = os.path.basename(example['audio_url'])\n",
    "    local_path = os.path.join(LOCAL_AUDIO_DIR, filename)\n",
    "    \n",
    "    if os.path.isfile(local_path):\n",
    "        try:\n",
    "            audio_array, sample_rate = sf.read(local_path)\n",
    "            if len(audio_array.shape) > 1:\n",
    "                audio_array = audio_array.mean(axis=1)\n",
    "            \n",
    "            # Limit audio length to prevent memory issues\n",
    "            max_samples = 16000 * 4  # 4 seconds max\n",
    "            if len(audio_array) > max_samples:\n",
    "                audio_array = audio_array[:max_samples]\n",
    "            \n",
    "            example['audio'] = {'array': audio_array, 'sampling_rate': sample_rate}\n",
    "        except Exception as e:\n",
    "            print(f\"Audio error {filename}: {e}\")\n",
    "            example['audio'] = None\n",
    "    else:\n",
    "        example['audio'] = None\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_pashto_dataset(PASHTO_DATA_JSON, max_samples=50)\n",
    "dataset = dataset.map(load_local_audio, with_indices=True)\n",
    "dataset = dataset.filter(lambda x: x['audio'] is not None)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(f\"Final dataset size: {len(dataset)}\")\n",
    "\n",
    "# CUDA-safe data collator\n",
    "@dataclass\n",
    "class CUDASafeDataCollator:\n",
    "    processor: CUDASafeSpeechT5Processor\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        valid_features = [f for f in features if f['audio'] is not None]\n",
    "        texts = [f[\"text\"] for f in valid_features]\n",
    "        \n",
    "        # Tokenize on CPU\n",
    "        text_inputs = self.processor.tokenizer(\n",
    "            texts, padding=True, truncation=True, \n",
    "            max_length=100, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Process audio with memory limits\n",
    "        max_audio_len = 16000 * 3  # 3 seconds max\n",
    "        audio_features = []\n",
    "        \n",
    "        for f in valid_features:\n",
    "            audio = torch.tensor(f[\"audio\"][\"array\"], dtype=torch.float32)\n",
    "            if len(audio) > max_audio_len:\n",
    "                audio = audio[:max_audio_len]\n",
    "            audio_features.append(audio)\n",
    "        \n",
    "        # Pad audio\n",
    "        if audio_features:\n",
    "            max_len = min(max([len(a) for a in audio_features]), max_audio_len)\n",
    "            padded_audio = []\n",
    "            \n",
    "            for audio in audio_features:\n",
    "                if len(audio) < max_len:\n",
    "                    padding = torch.zeros(max_len - len(audio))\n",
    "                    padded_audio.append(torch.cat([audio, padding]))\n",
    "                else:\n",
    "                    padded_audio.append(audio[:max_len])\n",
    "            \n",
    "            labels = torch.stack(padded_audio, dim=0)\n",
    "        else:\n",
    "            labels = torch.zeros(len(valid_features), 1000)\n",
    "        \n",
    "        speaker_embeddings = torch.zeros(len(valid_features), 512)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": text_inputs[\"input_ids\"],\n",
    "            \"attention_mask\": text_inputs[\"attention_mask\"], \n",
    "            \"labels\": labels,\n",
    "            \"speaker_embeddings\": speaker_embeddings,\n",
    "        }\n",
    "\n",
    "data_collator = CUDASafeDataCollator(processor=processor)\n",
    "\n",
    "# CUDA-optimized training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,  # Minimal batch size\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=1,\n",
    "    save_steps=10,\n",
    "    warmup_steps=2,\n",
    "    \n",
    "    # CUDA-specific settings\n",
    "    fp16=False,  # Disable mixed precision for stability\n",
    "    bf16=False,\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_num_workers=0,  # No multiprocessing\n",
    "    \n",
    "    # Memory management\n",
    "    max_grad_norm=0.5,  # Gradient clipping\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,\n",
    "    save_safetensors=False,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "# Create trainer with error handling\n",
    "print(\"Creating trainer...\")\n",
    "try:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    print(\"✅ Trainer created successfully\")\n",
    "    \n",
    "    # Clear cache before training\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(f\"GPU Memory before training: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "    \n",
    "    # Start training with monitoring\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"✅ Training completed!\")\n",
    "    \n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    print(f\"❌ CUDA OOM: {e}\")\n",
    "    print(\"Try reducing batch_size to 1 and gradient_accumulation_steps\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    if \"device-side assert\" in str(e):\n",
    "        print(f\"❌ CUDA device assert: {e}\")\n",
    "        print(\"Check token IDs are within vocabulary range\")\n",
    "    else:\n",
    "        print(f\"❌ Runtime error: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Save whatever we can\n",
    "    try:\n",
    "        print(\"Saving models...\")\n",
    "        trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "        processor.save_pretrained(os.path.join(OUTPUT_DIR, \"final_processor\"))\n",
    "        print(f\"✅ Models saved to {OUTPUT_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Save error: {e}\")\n",
    "    \n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Training process complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb01c547-e6fb-437f-82f9-3730194eaf68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch RTX 5060 Ti",
   "language": "python",
   "name": "pytorch_rtx5060"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
